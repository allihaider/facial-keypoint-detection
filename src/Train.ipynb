{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f350d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42dbba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-09 22:59:39.072200: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-09 22:59:39.072268: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-09 22:59:39.072591: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ali-Latitude-E5470): /proc/driver/nvidia/version does not exist\n",
      "2021-08-09 22:59:39.072997: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\", input_shape=(96, 96, 3))\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa34fa67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_1_conv/kernel:0' shape=(1, 1, 64, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_2), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_0_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_0_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_3), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_0_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_0_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_4), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_1_conv/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_5), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_6), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_7), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_1_conv/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_8), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_9), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_10), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_1_conv/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_11), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_12), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_13), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_0_conv/kernel:0' shape=(1, 1, 256, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_14), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_0_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_13), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_0_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_0_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_14), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_15), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_15), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_16), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_16), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_16), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_17), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_17), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_17), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_18), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_18), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_18), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_19), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_19), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_19), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_20), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_20), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_20), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_21), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_21), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_21), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_22), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_22), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_22), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_23), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_23), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_23), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_24), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_1_conv/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_24), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_24), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_25), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_25), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_25), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_26), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_26), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_27), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_0_conv/kernel:0' shape=(1, 1, 512, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_27), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_0_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_26), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_0_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_0_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_27), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_28), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_28), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_28), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_29), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_29), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_29), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_30), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_30), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_30), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_31), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_31), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_31), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_32), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_32), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_32), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_33), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_33), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_33), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_34), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_34), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_34), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_35), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_35), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_35), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_36), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_36), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_36), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_37), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_37), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_37), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_38), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_38), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_38), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_39), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_39), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_39), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_40), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_40), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_40), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_41), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_41), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_41), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_42), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_42), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_42), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_43), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_1_conv/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_43), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_43), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_44), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_44), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_44), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_45), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_45), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_46), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_0_conv/kernel:0' shape=(1, 1, 1024, 2048) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_46), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_0_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_45), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_0_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_0_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_46), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_47), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_1_conv/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_47), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_47), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_48), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_48), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_48), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_49), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_49), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_49), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_50), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_1_conv/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_50), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_50), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_51), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_51), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_51), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_52), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_52), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_52), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(96, 96, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(30)(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f67cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 96, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.pad (TFOpLambda)   (None, 102, 102, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution (TFOpLambda)  (None, 48, 48, 64)   0           tf.compat.v1.pad[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add (TFOpLambda)     (None, 48, 48, 64)   0           tf.nn.convolution[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 48, 48, 64), 0           tf.nn.bias_add[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu (TFOpLambda)         (None, 48, 48, 64)   0           tf.compat.v1.nn.fused_batch_norm[\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.pad_1 (TFOpLambda) (None, 50, 50, 64)   0           tf.nn.relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.max_pool (TFOpL (None, 24, 24, 64)   0           tf.compat.v1.pad_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_1 (TFOpLambda (None, 24, 24, 64)   0           tf.compat.v1.nn.max_pool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_1 (TFOpLambda)   (None, 24, 24, 64)   0           tf.nn.convolution_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 64), 0           tf.nn.bias_add_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_1 (TFOpLambda)       (None, 24, 24, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_2 (TFOpLambda (None, 24, 24, 64)   0           tf.nn.relu_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_2 (TFOpLambda)   (None, 24, 24, 64)   0           tf.nn.convolution_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 64), 0           tf.nn.bias_add_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_2 (TFOpLambda)       (None, 24, 24, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_4 (TFOpLambda (None, 24, 24, 256)  0           tf.compat.v1.nn.max_pool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_3 (TFOpLambda (None, 24, 24, 256)  0           tf.nn.relu_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_4 (TFOpLambda)   (None, 24, 24, 256)  0           tf.nn.convolution_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_3 (TFOpLambda)   (None, 24, 24, 256)  0           tf.nn.convolution_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 256) 0           tf.nn.bias_add_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 256) 0           tf.nn.bias_add_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 24, 24, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_3 (TFOpLambda)       (None, 24, 24, 256)  0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_5 (TFOpLambda (None, 24, 24, 64)   0           tf.nn.relu_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_5 (TFOpLambda)   (None, 24, 24, 64)   0           tf.nn.convolution_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 64), 0           tf.nn.bias_add_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_4 (TFOpLambda)       (None, 24, 24, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_6 (TFOpLambda (None, 24, 24, 64)   0           tf.nn.relu_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_6 (TFOpLambda)   (None, 24, 24, 64)   0           tf.nn.convolution_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 64), 0           tf.nn.bias_add_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_5 (TFOpLambda)       (None, 24, 24, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_7 (TFOpLambda (None, 24, 24, 256)  0           tf.nn.relu_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_7 (TFOpLambda)   (None, 24, 24, 256)  0           tf.nn.convolution_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 256) 0           tf.nn.bias_add_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 24, 24, 256)  0           tf.nn.relu_3[0][0]               \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_6 (TFOpLambda)       (None, 24, 24, 256)  0           tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_8 (TFOpLambda (None, 24, 24, 64)   0           tf.nn.relu_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_8 (TFOpLambda)   (None, 24, 24, 64)   0           tf.nn.convolution_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 64), 0           tf.nn.bias_add_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_7 (TFOpLambda)       (None, 24, 24, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_9 (TFOpLambda (None, 24, 24, 64)   0           tf.nn.relu_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_9 (TFOpLambda)   (None, 24, 24, 64)   0           tf.nn.convolution_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 64), 0           tf.nn.bias_add_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_8 (TFOpLambda)       (None, 24, 24, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_10 (TFOpLambd (None, 24, 24, 256)  0           tf.nn.relu_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_10 (TFOpLambda)  (None, 24, 24, 256)  0           tf.nn.convolution_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 24, 24, 256) 0           tf.nn.bias_add_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 24, 24, 256)  0           tf.nn.relu_6[0][0]               \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_9 (TFOpLambda)       (None, 24, 24, 256)  0           tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_11 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_11 (TFOpLambda)  (None, 12, 12, 128)  0           tf.nn.convolution_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.bias_add_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_10 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_12 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_12 (TFOpLambda)  (None, 12, 12, 128)  0           tf.nn.convolution_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.bias_add_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_11 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_14 (TFOpLambd (None, 12, 12, 512)  0           tf.nn.relu_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_13 (TFOpLambd (None, 12, 12, 512)  0           tf.nn.relu_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_14 (TFOpLambda)  (None, 12, 12, 512)  0           tf.nn.convolution_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_13 (TFOpLambda)  (None, 12, 12, 512)  0           tf.nn.convolution_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 512) 0           tf.nn.bias_add_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 512) 0           tf.nn.bias_add_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 12, 12, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_12 (TFOpLambda)      (None, 12, 12, 512)  0           tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_15 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_15 (TFOpLambda)  (None, 12, 12, 128)  0           tf.nn.convolution_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.bias_add_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_13 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_16 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_16 (TFOpLambda)  (None, 12, 12, 128)  0           tf.nn.convolution_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.bias_add_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_14 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_17 (TFOpLambd (None, 12, 12, 512)  0           tf.nn.relu_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_17 (TFOpLambda)  (None, 12, 12, 512)  0           tf.nn.convolution_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 512) 0           tf.nn.bias_add_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 12, 12, 512)  0           tf.nn.relu_12[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_15 (TFOpLambda)      (None, 12, 12, 512)  0           tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_18 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_18 (TFOpLambda)  (None, 12, 12, 128)  0           tf.nn.convolution_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.bias_add_18[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_16 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_19 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_19 (TFOpLambda)  (None, 12, 12, 128)  0           tf.nn.convolution_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.bias_add_19[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_17 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_20 (TFOpLambd (None, 12, 12, 512)  0           tf.nn.relu_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_20 (TFOpLambda)  (None, 12, 12, 512)  0           tf.nn.convolution_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 512) 0           tf.nn.bias_add_20[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 12, 12, 512)  0           tf.nn.relu_15[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_18 (TFOpLambda)      (None, 12, 12, 512)  0           tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_21 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_21 (TFOpLambda)  (None, 12, 12, 128)  0           tf.nn.convolution_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.bias_add_21[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_19 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_22 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_22 (TFOpLambda)  (None, 12, 12, 128)  0           tf.nn.convolution_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.bias_add_22[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_20 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_23 (TFOpLambd (None, 12, 12, 512)  0           tf.nn.relu_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_23 (TFOpLambda)  (None, 12, 12, 512)  0           tf.nn.convolution_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 512) 0           tf.nn.bias_add_23[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 12, 12, 512)  0           tf.nn.relu_18[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_21 (TFOpLambda)      (None, 12, 12, 512)  0           tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_24 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_24 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_24[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_22 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_25 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_25 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_25[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_23 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_27 (TFOpLambd (None, 6, 6, 1024)   0           tf.nn.relu_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_26 (TFOpLambd (None, 6, 6, 1024)   0           tf.nn.relu_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_27 (TFOpLambda)  (None, 6, 6, 1024)   0           tf.nn.convolution_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_26 (TFOpLambda)  (None, 6, 6, 1024)   0           tf.nn.convolution_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 1024), 0           tf.nn.bias_add_27[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 1024), 0           tf.nn.bias_add_26[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 6, 6, 1024)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_24 (TFOpLambda)      (None, 6, 6, 1024)   0           tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_28 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_28 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_28[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_25 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_29 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_29 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_29[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_26 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_30 (TFOpLambd (None, 6, 6, 1024)   0           tf.nn.relu_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_30 (TFOpLambda)  (None, 6, 6, 1024)   0           tf.nn.convolution_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 1024), 0           tf.nn.bias_add_30[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 6, 6, 1024)   0           tf.nn.relu_24[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_27 (TFOpLambda)      (None, 6, 6, 1024)   0           tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_31 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_31 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_31[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_28 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_32 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_32 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_32[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_29 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_33 (TFOpLambd (None, 6, 6, 1024)   0           tf.nn.relu_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_33 (TFOpLambda)  (None, 6, 6, 1024)   0           tf.nn.convolution_33[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 1024), 0           tf.nn.bias_add_33[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 6, 6, 1024)   0           tf.nn.relu_27[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_30 (TFOpLambda)      (None, 6, 6, 1024)   0           tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_34 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_34 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_34[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_34[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_31 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_35 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_35 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_35[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_35[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_32 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_36 (TFOpLambd (None, 6, 6, 1024)   0           tf.nn.relu_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_36 (TFOpLambda)  (None, 6, 6, 1024)   0           tf.nn.convolution_36[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 1024), 0           tf.nn.bias_add_36[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 6, 6, 1024)   0           tf.nn.relu_30[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_33 (TFOpLambda)      (None, 6, 6, 1024)   0           tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_37 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_37 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_37[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_37[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_34 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_38 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_38 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_38[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_35 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_39 (TFOpLambd (None, 6, 6, 1024)   0           tf.nn.relu_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_39 (TFOpLambda)  (None, 6, 6, 1024)   0           tf.nn.convolution_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 1024), 0           tf.nn.bias_add_39[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 6, 6, 1024)   0           tf.nn.relu_33[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_36 (TFOpLambda)      (None, 6, 6, 1024)   0           tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_40 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_40 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_40[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_37 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_41 (TFOpLambd (None, 6, 6, 256)    0           tf.nn.relu_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_41 (TFOpLambda)  (None, 6, 6, 256)    0           tf.nn.convolution_41[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 256),  0           tf.nn.bias_add_41[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_38 (TFOpLambda)      (None, 6, 6, 256)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_42 (TFOpLambd (None, 6, 6, 1024)   0           tf.nn.relu_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_42 (TFOpLambda)  (None, 6, 6, 1024)   0           tf.nn.convolution_42[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 6, 6, 1024), 0           tf.nn.bias_add_42[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, 6, 6, 1024)   0           tf.nn.relu_36[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_39 (TFOpLambda)      (None, 6, 6, 1024)   0           tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_43 (TFOpLambd (None, 3, 3, 512)    0           tf.nn.relu_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_43 (TFOpLambda)  (None, 3, 3, 512)    0           tf.nn.convolution_43[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 512),  0           tf.nn.bias_add_43[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_40 (TFOpLambda)      (None, 3, 3, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_44 (TFOpLambd (None, 3, 3, 512)    0           tf.nn.relu_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_44 (TFOpLambda)  (None, 3, 3, 512)    0           tf.nn.convolution_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 512),  0           tf.nn.bias_add_44[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_41 (TFOpLambda)      (None, 3, 3, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_46 (TFOpLambd (None, 3, 3, 2048)   0           tf.nn.relu_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_45 (TFOpLambd (None, 3, 3, 2048)   0           tf.nn.relu_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_46 (TFOpLambda)  (None, 3, 3, 2048)   0           tf.nn.convolution_46[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_45 (TFOpLambda)  (None, 3, 3, 2048)   0           tf.nn.convolution_45[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 2048), 0           tf.nn.bias_add_46[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 2048), 0           tf.nn.bias_add_45[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (None, 3, 3, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_42 (TFOpLambda)      (None, 3, 3, 2048)   0           tf.__operators__.add_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_47 (TFOpLambd (None, 3, 3, 512)    0           tf.nn.relu_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_47 (TFOpLambda)  (None, 3, 3, 512)    0           tf.nn.convolution_47[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 512),  0           tf.nn.bias_add_47[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_43 (TFOpLambda)      (None, 3, 3, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_48 (TFOpLambd (None, 3, 3, 512)    0           tf.nn.relu_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_48 (TFOpLambda)  (None, 3, 3, 512)    0           tf.nn.convolution_48[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 512),  0           tf.nn.bias_add_48[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_44 (TFOpLambda)      (None, 3, 3, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_49 (TFOpLambd (None, 3, 3, 2048)   0           tf.nn.relu_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_49 (TFOpLambda)  (None, 3, 3, 2048)   0           tf.nn.convolution_49[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 2048), 0           tf.nn.bias_add_49[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (None, 3, 3, 2048)   0           tf.nn.relu_42[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_45 (TFOpLambda)      (None, 3, 3, 2048)   0           tf.__operators__.add_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_50 (TFOpLambd (None, 3, 3, 512)    0           tf.nn.relu_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_50 (TFOpLambda)  (None, 3, 3, 512)    0           tf.nn.convolution_50[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 512),  0           tf.nn.bias_add_50[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_46 (TFOpLambda)      (None, 3, 3, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_51 (TFOpLambd (None, 3, 3, 512)    0           tf.nn.relu_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_51 (TFOpLambda)  (None, 3, 3, 512)    0           tf.nn.convolution_51[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 512),  0           tf.nn.bias_add_51[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_47 (TFOpLambda)      (None, 3, 3, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_52 (TFOpLambd (None, 3, 3, 2048)   0           tf.nn.relu_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_52 (TFOpLambda)  (None, 3, 3, 2048)   0           tf.nn.convolution_52[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 3, 3, 2048), 0           tf.nn.bias_add_52[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, 3, 3, 2048)   0           tf.nn.relu_45[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_48 (TFOpLambda)      (None, 3, 3, 2048)   0           tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           tf.nn.relu_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           61470       global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 61,470\n",
      "Trainable params: 61,470\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193e3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b983a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IdLookupTable.csv  SampleSubmission.csv  test.csv  training.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5419fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12895768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_eye_center_x</th>\n",
       "      <th>left_eye_center_y</th>\n",
       "      <th>right_eye_center_x</th>\n",
       "      <th>right_eye_center_y</th>\n",
       "      <th>left_eye_inner_corner_x</th>\n",
       "      <th>left_eye_inner_corner_y</th>\n",
       "      <th>left_eye_outer_corner_x</th>\n",
       "      <th>left_eye_outer_corner_y</th>\n",
       "      <th>right_eye_inner_corner_x</th>\n",
       "      <th>right_eye_inner_corner_y</th>\n",
       "      <th>...</th>\n",
       "      <th>nose_tip_y</th>\n",
       "      <th>mouth_left_corner_x</th>\n",
       "      <th>mouth_left_corner_y</th>\n",
       "      <th>mouth_right_corner_x</th>\n",
       "      <th>mouth_right_corner_y</th>\n",
       "      <th>mouth_center_top_lip_x</th>\n",
       "      <th>mouth_center_top_lip_y</th>\n",
       "      <th>mouth_center_bottom_lip_x</th>\n",
       "      <th>mouth_center_bottom_lip_y</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.033564</td>\n",
       "      <td>39.002274</td>\n",
       "      <td>30.227008</td>\n",
       "      <td>36.421678</td>\n",
       "      <td>59.582075</td>\n",
       "      <td>39.647423</td>\n",
       "      <td>73.130346</td>\n",
       "      <td>39.969997</td>\n",
       "      <td>36.356571</td>\n",
       "      <td>37.389402</td>\n",
       "      <td>...</td>\n",
       "      <td>57.066803</td>\n",
       "      <td>61.195308</td>\n",
       "      <td>79.970165</td>\n",
       "      <td>28.614496</td>\n",
       "      <td>77.388992</td>\n",
       "      <td>43.312602</td>\n",
       "      <td>72.935459</td>\n",
       "      <td>43.130707</td>\n",
       "      <td>84.485774</td>\n",
       "      <td>238 236 237 238 240 240 239 241 241 243 240 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64.332936</td>\n",
       "      <td>34.970077</td>\n",
       "      <td>29.949277</td>\n",
       "      <td>33.448715</td>\n",
       "      <td>58.856170</td>\n",
       "      <td>35.274349</td>\n",
       "      <td>70.722723</td>\n",
       "      <td>36.187166</td>\n",
       "      <td>36.034723</td>\n",
       "      <td>34.361532</td>\n",
       "      <td>...</td>\n",
       "      <td>55.660936</td>\n",
       "      <td>56.421447</td>\n",
       "      <td>76.352000</td>\n",
       "      <td>35.122383</td>\n",
       "      <td>76.047660</td>\n",
       "      <td>46.684596</td>\n",
       "      <td>70.266553</td>\n",
       "      <td>45.467915</td>\n",
       "      <td>85.480170</td>\n",
       "      <td>219 215 204 196 204 211 212 200 180 168 178 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.057053</td>\n",
       "      <td>34.909642</td>\n",
       "      <td>30.903789</td>\n",
       "      <td>34.909642</td>\n",
       "      <td>59.412000</td>\n",
       "      <td>36.320968</td>\n",
       "      <td>70.984421</td>\n",
       "      <td>36.320968</td>\n",
       "      <td>37.678105</td>\n",
       "      <td>36.320968</td>\n",
       "      <td>...</td>\n",
       "      <td>53.538947</td>\n",
       "      <td>60.822947</td>\n",
       "      <td>73.014316</td>\n",
       "      <td>33.726316</td>\n",
       "      <td>72.732000</td>\n",
       "      <td>47.274947</td>\n",
       "      <td>70.191789</td>\n",
       "      <td>47.274947</td>\n",
       "      <td>78.659368</td>\n",
       "      <td>144 142 159 180 188 188 184 180 167 132 84 59 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65.225739</td>\n",
       "      <td>37.261774</td>\n",
       "      <td>32.023096</td>\n",
       "      <td>37.261774</td>\n",
       "      <td>60.003339</td>\n",
       "      <td>39.127179</td>\n",
       "      <td>72.314713</td>\n",
       "      <td>38.380967</td>\n",
       "      <td>37.618643</td>\n",
       "      <td>38.754115</td>\n",
       "      <td>...</td>\n",
       "      <td>54.166539</td>\n",
       "      <td>65.598887</td>\n",
       "      <td>72.703722</td>\n",
       "      <td>37.245496</td>\n",
       "      <td>74.195478</td>\n",
       "      <td>50.303165</td>\n",
       "      <td>70.091687</td>\n",
       "      <td>51.561183</td>\n",
       "      <td>78.268383</td>\n",
       "      <td>193 192 193 194 194 194 193 192 168 111 50 12 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.725301</td>\n",
       "      <td>39.621261</td>\n",
       "      <td>32.244810</td>\n",
       "      <td>38.042032</td>\n",
       "      <td>58.565890</td>\n",
       "      <td>39.621261</td>\n",
       "      <td>72.515926</td>\n",
       "      <td>39.884466</td>\n",
       "      <td>36.982380</td>\n",
       "      <td>39.094852</td>\n",
       "      <td>...</td>\n",
       "      <td>64.889521</td>\n",
       "      <td>60.671411</td>\n",
       "      <td>77.523239</td>\n",
       "      <td>31.191755</td>\n",
       "      <td>76.997301</td>\n",
       "      <td>44.962748</td>\n",
       "      <td>73.707387</td>\n",
       "      <td>44.227141</td>\n",
       "      <td>86.871166</td>\n",
       "      <td>147 148 160 196 215 214 216 217 219 220 206 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7044</th>\n",
       "      <td>67.402546</td>\n",
       "      <td>31.842551</td>\n",
       "      <td>29.746749</td>\n",
       "      <td>38.632942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>67.029093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.426637</td>\n",
       "      <td>79.683921</td>\n",
       "      <td>71 74 85 105 116 128 139 150 170 187 201 209 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>66.134400</td>\n",
       "      <td>38.365501</td>\n",
       "      <td>30.478626</td>\n",
       "      <td>39.950198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>66.626011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.287397</td>\n",
       "      <td>77.983023</td>\n",
       "      <td>60 60 62 57 55 51 49 48 50 53 56 56 106 89 77 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7046</th>\n",
       "      <td>66.690732</td>\n",
       "      <td>36.845221</td>\n",
       "      <td>31.666420</td>\n",
       "      <td>39.685042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>67.515161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.462572</td>\n",
       "      <td>78.117120</td>\n",
       "      <td>74 74 74 78 79 79 79 81 77 78 80 73 72 81 77 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7047</th>\n",
       "      <td>70.965082</td>\n",
       "      <td>39.853666</td>\n",
       "      <td>30.543285</td>\n",
       "      <td>40.772339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>66.724988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.065186</td>\n",
       "      <td>79.586447</td>\n",
       "      <td>254 254 254 254 254 238 193 145 121 118 119 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>66.938311</td>\n",
       "      <td>43.424510</td>\n",
       "      <td>31.096059</td>\n",
       "      <td>39.528604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>73.033339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.900480</td>\n",
       "      <td>82.773096</td>\n",
       "      <td>53 62 67 76 86 91 97 105 105 106 107 108 112 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7049 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      left_eye_center_x  left_eye_center_y  right_eye_center_x  \\\n",
       "0             66.033564          39.002274           30.227008   \n",
       "1             64.332936          34.970077           29.949277   \n",
       "2             65.057053          34.909642           30.903789   \n",
       "3             65.225739          37.261774           32.023096   \n",
       "4             66.725301          39.621261           32.244810   \n",
       "...                 ...                ...                 ...   \n",
       "7044          67.402546          31.842551           29.746749   \n",
       "7045          66.134400          38.365501           30.478626   \n",
       "7046          66.690732          36.845221           31.666420   \n",
       "7047          70.965082          39.853666           30.543285   \n",
       "7048          66.938311          43.424510           31.096059   \n",
       "\n",
       "      right_eye_center_y  left_eye_inner_corner_x  left_eye_inner_corner_y  \\\n",
       "0              36.421678                59.582075                39.647423   \n",
       "1              33.448715                58.856170                35.274349   \n",
       "2              34.909642                59.412000                36.320968   \n",
       "3              37.261774                60.003339                39.127179   \n",
       "4              38.042032                58.565890                39.621261   \n",
       "...                  ...                      ...                      ...   \n",
       "7044           38.632942                 0.000000                 0.000000   \n",
       "7045           39.950198                 0.000000                 0.000000   \n",
       "7046           39.685042                 0.000000                 0.000000   \n",
       "7047           40.772339                 0.000000                 0.000000   \n",
       "7048           39.528604                 0.000000                 0.000000   \n",
       "\n",
       "      left_eye_outer_corner_x  left_eye_outer_corner_y  \\\n",
       "0                   73.130346                39.969997   \n",
       "1                   70.722723                36.187166   \n",
       "2                   70.984421                36.320968   \n",
       "3                   72.314713                38.380967   \n",
       "4                   72.515926                39.884466   \n",
       "...                       ...                      ...   \n",
       "7044                 0.000000                 0.000000   \n",
       "7045                 0.000000                 0.000000   \n",
       "7046                 0.000000                 0.000000   \n",
       "7047                 0.000000                 0.000000   \n",
       "7048                 0.000000                 0.000000   \n",
       "\n",
       "      right_eye_inner_corner_x  right_eye_inner_corner_y  ...  nose_tip_y  \\\n",
       "0                    36.356571                 37.389402  ...   57.066803   \n",
       "1                    36.034723                 34.361532  ...   55.660936   \n",
       "2                    37.678105                 36.320968  ...   53.538947   \n",
       "3                    37.618643                 38.754115  ...   54.166539   \n",
       "4                    36.982380                 39.094852  ...   64.889521   \n",
       "...                        ...                       ...  ...         ...   \n",
       "7044                  0.000000                  0.000000  ...   67.029093   \n",
       "7045                  0.000000                  0.000000  ...   66.626011   \n",
       "7046                  0.000000                  0.000000  ...   67.515161   \n",
       "7047                  0.000000                  0.000000  ...   66.724988   \n",
       "7048                  0.000000                  0.000000  ...   73.033339   \n",
       "\n",
       "      mouth_left_corner_x  mouth_left_corner_y  mouth_right_corner_x  \\\n",
       "0               61.195308            79.970165             28.614496   \n",
       "1               56.421447            76.352000             35.122383   \n",
       "2               60.822947            73.014316             33.726316   \n",
       "3               65.598887            72.703722             37.245496   \n",
       "4               60.671411            77.523239             31.191755   \n",
       "...                   ...                  ...                   ...   \n",
       "7044             0.000000             0.000000              0.000000   \n",
       "7045             0.000000             0.000000              0.000000   \n",
       "7046             0.000000             0.000000              0.000000   \n",
       "7047             0.000000             0.000000              0.000000   \n",
       "7048             0.000000             0.000000              0.000000   \n",
       "\n",
       "      mouth_right_corner_y  mouth_center_top_lip_x  mouth_center_top_lip_y  \\\n",
       "0                77.388992               43.312602               72.935459   \n",
       "1                76.047660               46.684596               70.266553   \n",
       "2                72.732000               47.274947               70.191789   \n",
       "3                74.195478               50.303165               70.091687   \n",
       "4                76.997301               44.962748               73.707387   \n",
       "...                    ...                     ...                     ...   \n",
       "7044              0.000000                0.000000                0.000000   \n",
       "7045              0.000000                0.000000                0.000000   \n",
       "7046              0.000000                0.000000                0.000000   \n",
       "7047              0.000000                0.000000                0.000000   \n",
       "7048              0.000000                0.000000                0.000000   \n",
       "\n",
       "      mouth_center_bottom_lip_x  mouth_center_bottom_lip_y  \\\n",
       "0                     43.130707                  84.485774   \n",
       "1                     45.467915                  85.480170   \n",
       "2                     47.274947                  78.659368   \n",
       "3                     51.561183                  78.268383   \n",
       "4                     44.227141                  86.871166   \n",
       "...                         ...                        ...   \n",
       "7044                  50.426637                  79.683921   \n",
       "7045                  50.287397                  77.983023   \n",
       "7046                  49.462572                  78.117120   \n",
       "7047                  50.065186                  79.586447   \n",
       "7048                  45.900480                  82.773096   \n",
       "\n",
       "                                                  Image  \n",
       "0     238 236 237 238 240 240 239 241 241 243 240 23...  \n",
       "1     219 215 204 196 204 211 212 200 180 168 178 19...  \n",
       "2     144 142 159 180 188 188 184 180 167 132 84 59 ...  \n",
       "3     193 192 193 194 194 194 193 192 168 111 50 12 ...  \n",
       "4     147 148 160 196 215 214 216 217 219 220 206 18...  \n",
       "...                                                 ...  \n",
       "7044  71 74 85 105 116 128 139 150 170 187 201 209 2...  \n",
       "7045  60 60 62 57 55 51 49 48 50 53 56 56 106 89 77 ...  \n",
       "7046  74 74 74 78 79 79 79 81 77 78 80 73 72 81 77 1...  \n",
       "7047  254 254 254 254 254 238 193 145 121 118 119 10...  \n",
       "7048  53 62 67 76 86 91 97 105 105 106 107 108 112 1...  \n",
       "\n",
       "[7049 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14545e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rows, no_columns = train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29c727a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s = []\n",
    "for i in range(no_rows):\n",
    "    y_s.append(train_df.iloc[i][:-1].values.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed25e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_image_from_string(image_pixels_string):\n",
    "    \n",
    "    image_pixels_list = [int(point) for point in image_pixels_string.split(\" \")]\n",
    "    image_side = int(np.sqrt(len(image_pixels_list)))\n",
    "    image = np.reshape(image_pixels_list, (image_side, image_side))\n",
    "    image = np.stack([image, image, image], axis=2)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = []\n",
    "for i in range(no_rows):\n",
    "    x_s.append(get_image_from_string(train_df.iloc[i][\"Image\"]).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc353e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = np.empty((7049,96,96, 3))\n",
    "for i in range(len(x_s)):\n",
    "    x_batch[i,:,:] = x_s[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch = np.empty((7049, 30))\n",
    "for i in range(len(y_s)):\n",
    "    y_batch[i,:] = y_s[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "21547465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-01 20:35:55.716536: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 779563008 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 88/221 [==========>...................] - ETA: 58s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8252/2864771929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m               loss='mean_squared_error')\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Kaggle-facial-keypoints/facial-keypoints-env/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Kaggle-facial-keypoints/facial-keypoints-env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Kaggle-facial-keypoints/facial-keypoints-env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Kaggle-facial-keypoints/facial-keypoints-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Kaggle-facial-keypoints/facial-keypoints-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Desktop/Kaggle-facial-keypoints/facial-keypoints-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Kaggle-facial-keypoints/facial-keypoints-env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "model.fit(x_batch, y_batch, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack([x_s[0], x_s[0], x_s[0]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtorgb = cv.cvtColor(np.expand_dims(x_s[0], axis=2), cv.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d2e997d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 96)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2c2968f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 96, 1)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be66e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facial-keypoints-env",
   "language": "python",
   "name": "facial-keypoints-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
